<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>A linear regression with correlated residuals | Charles Perretti</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">A linear regression with correlated residuals</span></h1>

<h2 class="date">2019/01/08</h2>
</div>

<main>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>One of the central assumptions of the linear regression is that the residuals are uncorrelated. Unfortunately, this assumption is often violated in real data. As we’ll see, ignoring this can lead to incorrect inferences, primarily through overconfident estimates. Here I’ll demonstrate how to fit a linear regression that accounts for correlated residuals, and how the results differ from that of a linear regression without correlated residuals (which I’ll call a “simple linear regression”). I’ll do this in R using Template Model Builder (<a href="https://github.com/kaskr/adcomp">TMB</a>).</p>
<p>To start, let’s say we have a time series that is made up of an underlying linear trend with added observation error that is correlated over time. This means that if the error is above the trend line in a particular year it tends to be above the trend in the next year, and vice versa. In equations we can represent this as,</p>
<p><span class="math display">\[y_{t} = \beta_0 + \beta_1t + \omega_t\]</span></p>
<p><span class="math display">\[\omega_t = \phi \omega_{t-1} + \epsilon_t\]</span> <span class="math display">\[\epsilon_t \sim N(0, \sigma)\]</span> Where <span class="math inline">\(y_t\)</span> is the observed time series, <span class="math inline">\(t\)</span> is the time index, and <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the intercept and slope of the trend, respectively. The interesting action is in <span class="math inline">\(\omega_t\)</span>. For a simple linear regression <span class="math inline">\(\omega_t\)</span> would just be normal, uncorrelated random error (<span class="math inline">\(\epsilon_t\)</span>). But the <span class="math inline">\(\phi \omega_{t-1}\)</span> allows for correlated errors, where this year’s error is partially determined by last year’s error. The parameter <span class="math inline">\(\phi\)</span> determines the correlation of the residuals: at <span class="math inline">\(\phi = 1\)</span> the errors are perfectly positively correlated, at <span class="math inline">\(\phi = 0\)</span> the model converges to a simple linear regression, and at <span class="math inline">\(\phi = -1\)</span> the errors are perfectly negatively correlated.</p>
</div>
<div id="simulation" class="section level2">
<h2>Simulation</h2>
<p>Next, let’s simulate an example time series with a long-term positive trend and correlated observation errors.</p>
<pre class="r"><code># load required libraries
library(ggplot2)
library(TMB)</code></pre>
<pre><code>## Warning in checkMatrixPackageVersion(): Package version inconsistency detected.
## TMB was built with Matrix version 1.2.15
## Current Matrix version is 1.2.14
## Please re-install &#39;TMB&#39; from source using install.packages(&#39;TMB&#39;, type = &#39;source&#39;) or ask CRAN for a binary version of &#39;TMB&#39; matching CRAN&#39;s &#39;Matrix&#39; package</code></pre>
<pre class="r"><code>library(dplyr)

set.seed(123) # for reproducibility

n_t &lt;- 30 # length of time series

# Create trend
Year &lt;- 1:n_t
b0 &lt;- 1
b1 &lt;- 0.1
trend &lt;- b0 + b1 * Year

# Create errors
e &lt;- vector(mode = &quot;numeric&quot;, length = n_t) # vector for errors
sigma &lt;- 1 # sd of white noise component
e[1] &lt;- rnorm(1, sd = sigma) # error in first year
phi &lt;- 0.9 # autocorrelation parameter
for (i in 2:n_t) e[i] &lt;- phi * e[i-1] + rnorm(1, sd = sigma) # create errors

# Add errors to trend
Observation &lt;- trend + e

# Plot it
ggplot(data.frame(Year = Year, Observation = Observation, trend = trend),
       aes(x = Year)) +
  geom_point(aes(y = Observation)) +
  geom_line(aes(y = trend)) +
  theme_bw() +
  ylab(&quot;Observation value&quot;) +
  theme(axis.title = element_text(size = 14))</code></pre>
<p><img src="/post/lm_ar_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Here, the trend line is the true trend, not a fit. Notice that the observations tend to be above the trend line in the early years, and then below it towards the end. This is the correlated error at work. The deviations from the long-term trend are just temporary and will eventually revert back to the trend. In a real system there may be some mechanism driving the correlation, and you could try to model that explicitly. Here, however, we’ll treat it as random noise and focus instead on getting an accurate estimate of the long-term trend.</p>
</div>
<div id="fit-the-model" class="section level2">
<h2>Fit the model</h2>
<p>Next, let’s look at the TMB model file that we’ll use to fit a linear regression with correlated residuals. It begins with the usual preamble of loading the required c++ libraries.</p>
<pre class="r"><code>writeLines(readLines(&quot;lm_ar.cpp&quot;)[1:3])</code></pre>
<pre><code>#include &lt;TMB.hpp&gt;
using namespace density;</code></pre>
<p>Next, since the correlation of the error must be between -1 and 1, we define a function called <code>bound</code> that bounds the <span class="math inline">\(\phi\)</span> parameter between -1 and 1. Then we define the data and parameters.</p>
<pre class="r"><code>writeLines(readLines(&quot;lm_ar.cpp&quot;)[4:18])</code></pre>
<pre><code>template &lt;class Type&gt; // define function to bound phi between -1 and 1
Type bound(Type x){return Type(2)/(Type(1) + exp(-Type(2) * x)) - Type(1);}

template&lt;class Type&gt;
Type objective_function&lt;Type&gt;::operator() ()
{
  // Data
  DATA_VECTOR(x);
  DATA_VECTOR(y);
  
  // Parameters
  PARAMETER(b0);
  PARAMETER(b1);
  PARAMETER(unbounded_phi);
  PARAMETER(log_ar_sd);</code></pre>
<p>The parameter <span class="math inline">\(\sigma\)</span> is strictly positive, and <span class="math inline">\(\phi\)</span> is bounded between -1 and 1, but we want TMB to be able to search over an unbounded range to avoid numerical problems. So we input them as log-transformed (<code>log_ar_sd</code>) and unbounded (<code>unbounded_phi</code>) parameters, respectively. Then we transform them to their bounded range before applying them in the model.</p>
<pre class="r"><code>writeLines(readLines(&quot;lm_ar.cpp&quot;)[20:22])</code></pre>
<pre><code>  // Transform variables
  Type ar_sd = exp(log_ar_sd);
  Type phi = bound(unbounded_phi);</code></pre>
<p>Next we generate the model fit which is just the predictions from the simple linear regression.</p>
<pre class="r"><code>writeLines(readLines(&quot;lm_ar.cpp&quot;)[25:28])</code></pre>
<pre><code>  // Generate model fit
  int n_y = y.size(); // number of observations
  vector&lt;Type&gt; fit_y(n_y); // vector for fit
  fit_y = b0 + b1 * x; // make fit</code></pre>
<p>So far, this is identical to how you would code a simple linear regression except for defining <span class="math inline">\(\phi\)</span> and its transformation. The big difference comes next, in the likelihood. Rather than using the normal distribution to evaluate the likelihood of the data (i.e., <code>dnorm()</code>), we’ll use the <code>AR1()</code> distribution. Remember that a call of <code>dnorm(x, 0, 1)</code> gives you the likelihood of the data <code>x</code> given that it follows a normal distribution with mean = 0 and sd = 1. Similarly, a call of <code>AR1(phi)(x)</code> gives you the likelihood of the data <code>x</code> given that it follows an AR1 process with correlation <code>phi</code>. Notice that the syntax is different from <code>dnorm()</code>: here <code>x</code> is in its own parentheses. The default call of <code>AR1(phi)(x)</code> assumes that the AR1 process has sd = 1, so it won’t fit the sd. We use the <code>SCALE()</code> function to fit the sd. So to evaluate the likelihood of the data <code>x</code> given it is from an AR1 process with correlation <code>phi</code> and unknown sd, you would write <code>SCALE(AR1(phi), ar_sd)(x)</code>, where <code>ar_sd</code> is the sd parameter.</p>
<p>Here it is in our code:</p>
<pre class="r"><code>writeLines(readLines(&quot;lm_ar.cpp&quot;)[31:33])</code></pre>
<pre><code>  // Likelihood of observations
  Type jnll = 0;
  jnll += SCALE(AR1(phi), ar_sd)(y - fit_y); //already negative and logged </code></pre>
<p>Lastly we report our fit line, parameters, and likelihood value.</p>
<pre class="r"><code>writeLines(readLines(&quot;lm_ar.cpp&quot;)[35:43])</code></pre>
<pre><code>  // Reporting
  ADREPORT(fit_y);
  ADREPORT(b0);
  ADREPORT(b1);
  ADREPORT(unbounded_phi);
  ADREPORT(log_ar_sd);
  
  return jnll;
  </code></pre>
<p>Okay, next let’s fit the model to the simulated data. For comparison, we’ll also fit a simple linear regression. We’ll use the <code>map</code> argument in the TMB function <code>MakeADFun</code> to fit both models. The <code>map</code> argument allows you to set a certain parameter to a fixed value. In this case, setting <code>phi = 0</code> yields the simple linear regression.</p>
<p>First we compile and load the model.</p>
<pre class="r"><code># Compile model
compile(&quot;lm_ar.cpp&quot;)</code></pre>
<pre><code>## Note: Using Makevars in /Users/charlesperretti/.R/Makevars</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code># Build object
dyn.load(dynlib(&quot;lm_ar&quot;))</code></pre>
<p>Then we fit the two regressions.</p>
<pre class="r"><code>Obj_lmar &lt;- MakeADFun(data = list(&quot;x&quot; = Year, # regression with correlated residuals
                                  &quot;y&quot; = Observation), 
                      parameters =  list(&quot;b0&quot; = 0,
                                         &quot;b1&quot; = 0,
                                         &quot;unbounded_phi&quot; = 0,
                                         &quot;log_ar_sd&quot; = 0))

Obj_lm &lt;- MakeADFun(data = list(&quot;x&quot; = Year, # simple regression
                                &quot;y&quot; = Observation), 
                    parameters =  list(&quot;b0&quot; = 0,
                                       &quot;b1&quot; = 0,
                                       &quot;unbounded_phi&quot; = 0,
                                       &quot;log_ar_sd&quot; = 0),
                    map = list(&quot;unbounded_phi&quot; = factor(NA)))


# Optimize
Opt_lmar &lt;- TMBhelper::Optimize(obj = Obj_lmar, newtonsteps = 1)
Opt_lm &lt;- TMBhelper::Optimize(obj = Obj_lm, newtonsteps = 1)

# Report out the ADREPORT vars
Report_sd_lm &lt;- TMB::sdreport(Obj_lm)
Report_sd_lmar &lt;- TMB::sdreport(Obj_lmar)</code></pre>
<p>Notice the two calls to <code>MakeADFun</code> to set up each model. The only difference between the two is that in the simple regression (the second one), the <code>phi</code> parameter is set to 0. We do this through the <code>map</code> argument with <code>unbounded_phi =  factor(NA)</code>. This tells TMB to keep <code>unbounded_phi</code> set to its initial guess which is given in the <code>parameters</code> argument (i.e., <code>unbounded_phi = 0</code>).</p>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p>Next let’s look at each regression against the true trend and observations. I’ll hide the plotting code for brevity but if you’re interested you can find it <a href="https://github.com/perretti/site/blob/master/content/post/lm_ar.Rmd">here</a>.</p>
<p><img src="/post/lm_ar_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Notice that the true trend is upward, while the simple regression fit (<code>fit_lm</code>) is downward, and the regression with correlated residuals (<code>fit_lmar</code>) is flat. So while the correlated regression is less wrong, neither is doing particularly well at estimating the true trend. But the difference between the two is that the regression with correlated residuals has a <em>much</em> larger confidence interval, and although it gets the trend wrong, its confidence interval at least captures the true trend. The simple regression on the other hand has a much tighter confidence interval which doesn’t capture the true trend. In my experience, this is a general pattern when comparing these two types of regression. The regression with correlated residuals acknowledges that the observations are not totally independent, therefore there is less information with which to estimate the trend, and as a result the confidence interval is expanded. This is the more honest depiction of the uncertainty.</p>
<p>Lastly, let’s take a look at how well the correlated regression model estimates the parameters of the simulation model.</p>
<p><img src="/post/lm_ar_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>We see that true parameter values (in red) are within the 95% confidence interval of the estimates (blue) for all parameters, suggesting that the model is working as expected.</p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>Hopefully you’ve learned how to fit a regression with correlated residuals in R using TMB. Along the way we went over how to set parameters to a fixed value in TMB using the <code>map</code> argument, and how to bound parameters within certain intervals. We’ve also shown how the results of a regression with correlated residuals differs from a simple regression, particularly in the size of the confidence interval.</p>
</div>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  
  <hr/>
  <a href="https://github.com/perretti">Github</a> | <a href="https://twitter.com/charlieperretti">Twitter</a>
  
  </footer>
  </body>
</html>

